# Hands-on: Integração com Kafka

## Pré-requisitos
- Acesso ao ambiente Databricks
- Conhecimento básico de Kafka
- dbdemos instalado

## Roteiro de Execução

### Será necessario criar uma Pipeline DLT nas etapas 1 e 2, esse guia pode ser encontrado em: <a href="$./Kafka/001_read_kafka">001_read_kafka</a>

### 1. Leitura de Dados do Kafka com SQL
**Notebook**: <a href="$./Kafka/001_read_kafka">001_read_kafka</a>
- Configuração de conexão com Kafka
- Leitura de tópicos usando SQL
- Transformações e processamento

### 2. Streaming Estruturado com Kafka
**Notebook**: <a href="$./Kafka/001_read_kafka_structured_streaming">001_read_kafka_structured_streaming</a>
- Implementação de Structured Streaming
- Processamento em tempo real
- Gerenciamento de checkpoints

### 3. Integração com PySpark
**Notebook**: <a href="$./Kafka/001_read_kafka_pyspark">001_read_kafka_pyspark</a>
- Leitura de dados usando PySpark
- Transformações avançadas
- Boas práticas de integração

## Recursos Adicionais
- Documentação oficial do Kafka
- Exemplos práticos de casos de uso
- Dicas de otimização

## Observações Importantes
- Execute os notebooks na ordem apresentada
- Instruções detalhadas em português
- Limpe os recursos após concluir os exercícios

Para retornar ao índice dos laboratórios, clique <a href="$./README.md">aqui</a>. 